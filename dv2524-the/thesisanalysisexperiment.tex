% thesisanalysisexperiment.tex
% Chapter Analysis, Experiment

% Analysis, Experiment
\chapter{Analysis, Experiment}
\label{cha:analysisexperiment}
Based off the reference profiling presented in figure \ref{fig:histogramshost}, we may conclude that the benchmarks, when hardware accelerated on the \dvttermhost\ system, perform with concentrated density; not being much scattered accross the graph except a few irregular extremeties in terms of maximum frametimes (see table \ref{tab:keyvalhost}).

Furthermore, we may conclude that the Phong demo features two distinct peaks in density distribution - about $1$ ms in-between.
There is cause to believe that this may be caused, or partly caused, by frame-wize rotation of the teapot - rotation featured in the benchmark (see section \ref{sec:methodologyexperiment_benchmarking}) - inducing some fluctutation into it's execution (see figure \ref{fig:histogramssimicsparaphong}).
However, this behaviour is, strangely so, not apperent whilst paravirtualized in the \dvttermqemu\ derived \dvttermandroidemulator , although this may be a visual artifact due to the resolution of the graph (see figure \ref{fig:histogramsqemu}).
See section \ref{sec:analysisexperiment_benchmarkvariations} for an elaboration on divergency in the Phong benchmark.

Additionally, and in accordance to tables \ref{tab:keyvalhost} and \ref{tab:keyvalqemu}, one may observe relatively high recorded maximum frametimes in relation to compiled maximum- and average values, yet featuring - in relation to divergent maximum, relatively low standard deviations.\\

\noindent
In this chapter, an analysis of the benchmarks, performed for the purpose of this experiment, in software rasterized- and paravirtualized \dvttermsimics\ platforms is presented and segmented into paragraphs for each benchmark.
Said paragraphs are presented below.

\paragraph{Chess}
\label{par:analysisexperiment_chess}
From the data visualized in figure \ref{fig:histogramssimicsparachess}, we may observe that the Chess benchmark, when executed in the software rasterized \dvttermsimics\ platform, has a relatively broad distribution of it's sample density, yet the distribution often seems evenly distributed around a singe point\todo{Confirm this by analyzing and presenting the benchmark mean.}.
The right-hand side of the graph, although also showcasing the impaired performance of the corresponding - paravirtualized - platform, visualises a decrease in the distribution of the sample density.
This is supported by the data presented in table \ref{tab:keyvalpara}.

Based on the data summarized in table \ref{tab:keyvalsimics} (whilst software rasterized in \dvttermsimics ) and comparing said data to that of table \ref{tab:keyvalpara} (whilst paravirtualized in \dvttermsimics ), we may observe that the software rasterized solution outperforms it's paravirtualized counterpart; not only in the base experiment, but in all of it's variations.
The only redeeming attributes the paravirtualized solution brings to the table, as elaborated upon in the above paragraph, is a decrease in the standard deviation of the benchmark profiling.
When comparing these results to the uncompromised hardware accelerated counterpart on the \dvttermhost\ machine (see figure \ref{fig:histogramshost}), we may observe - albeit considerably less prominent - an adherence to the single-peak behaviour in the destribution of the sample density.

The purpose of the Chess benchmark was to locate any bottlenecks related to the number of paravirtualized library invocations, which was predicted during the pilot study performed for the sake of this experiment (see \dvtcmdcitefur{dissertation:nilsson:2014}).
As such, there is cause to believe that the prediction of a probable bottleneck in the \dvttermtarget - to \dvttermhost\ communication latency has been confirmed; arguably identifying the weakness of graphics paravirtualization in the \dvttermsimics\ full-system simulator.

The conclusion drawn from the Chess benchmark data presented in chapter \ref{cha:results}, stresses further analysis into what is the root cause for the \dvttermtarget - to \dvttermhost\ latency for a multitude of paravirtualized method invocations.
Some suspicions related to this matter are presented in section \ref{sec:analysisexperiment_magicinstructionoverhead}. 

% TODO:
% Mention the number of magic instructions.

\paragraph{Julia}
\label{par:analysisexperiment_julia}
In figure \ref{fig:histogramssimicsparajulia}, we may observe double- to triple peak behaviour in the distribution of the sample density; both in software rasterized- and paravirtualized platforms.
Albeit the hardware accelerated \dvttermhost\ profiling (see figure \ref{fig:histogramshost}) may, however minor, suggest such a pattern; it is by all means not significant.
We may observe similar behaviour in the distribution of the sample density when profiling the same benchmark whilst paravirtualized in the \dvttermqemu -derived \dvttermandroidemulator\ (see figure \ref{fig:histogramsqemu}).
What causes this behaviour is unclear, as frame-to-frame branching in the fractal algorithm is minor and ought not cause such a variance.

The reason as to why the Julia benchmark was decided upon to be integrated into experiment outlined during the pilot study (see \dvtcmdcitefur{dissertation:nilsson:2014}) was to establish how the paravirtualized solution performed under computational stress.
Using this benchmark, a performance weakness in the software rasterized \dvttermsimics\ platform has been identified, with frametimes well above the two second mark (see table \ref{tab:keyvalsimics}); with the corresponding maximum frametime in the paravirtualized \dvttermsimics\ platform measuring up to to \dvtcmdfirstline{parajulia900.dat.max} ms.
With the Julia benchmark, as visualized in figure \ref{fig:histogramssimicsparajulia}, we've showcased radical performance improvements for computationally intensive graphics kernels, and - in turn - possibly identified the capabilities of graphics paravirtualization in the \dvttermsimics\ full-system simulator\todo{Expand upon scalability of Julia paravirtualization.}.

% TODO:
% Percentage-wize, the standard deviation has increased in the paravirtualized platform?
% Scalability
% Visualize how the Julia benchmark performed on the host with variations - in order to establish that the paravirtualized solution scales equally well - being inhibited only by overhead.

\paragraph{Phong}
\label{par:analysisexperiment_phong}
\begin{alltt}
DISCLAIMER: The Phong benchmark signals strange behaviour when software rasterized on the simulation \dvttermhost , and is to be considered less reliable (see section \ref{sec:analysisexperiment_benchmarkvariations}).
\end{alltt}

\noindent
The Phong benchmark was incorporated into this study for the purposes of analyzing the performance of stressed bandwidth in \dvttermtarget - to \dvttermhost\ communications; featuring relaying and rendering of large texture to simulation \dvttermhost .
The graphs presented in figure \ref{fig:histogramssimicsparaphong} display erratic distribution of the sample density in both software rasterized- and paravirtualized \dvttermsimics\ platforms.
Suspicions as to why this is the case are presented in section \ref{sec:analysisexperiment_benchmarkvariations}.
Furthermore, one might argue that parts of the distribution align with the double-peak density distribution showcased by the benchmark when hardware accelerated on the simulation \dvttermhost\ (see figure \ref{fig:histogramshost}).

% TODO:
% Elaborate into what may cause this cost. Memory page translation etc.

By analyzing the data in presented in tables \ref{tab:keyvalsimics} and \ref{tab:keyvalpara}, it is clear that the Phong benchmark, in terms of average framtimes, performs only marginally better or competatively to it's software rasterized equivalent.
However, and more interestingly so, executing the benchmark in the paravirtualized \dvttermsimics\ environment results in major improvements in terms of frametime maximum and standard deviation; topping in at \dvtcmdfirstline{simicsphong1448x1448.dat.max} ms where the corresponding paravirtualized maximum is but \dvtcmdfirstline{paraphong1448x1448.dat.max} ms.

What causes the compiled paravirtualized Phong benchmark samples to be only marginally better or competitively to their the software rasterized counerparts is unclear.
The effect could be related to a bottleneck in the memory page traversal, as described in section \ref{sec:methodologysolution_memorytabletraversal}, signifying a possible weakness in - not only the communicational latency, as established in paragraph \dvtcmdrefname{par:analysisexperiment_chess} - in the memory bandwidth of the Simics Pipe (see section \ref{sec:methodologysolution_simicspipe}).
However, in line with the recorded competatitive average- and minimum frametimes  - yet major improvements in frametime maximum and standard deviation - one might argue that the lightweight benchmark is misrepresentative in that it's software rasterized execution is below that of the overhead induced by paravirtualization.

However, due to the deviances described in section \ref{sec:analysisexperiment_benchmarkvariations}, this will not be elaborated upon further for the remainder of this dissertation.

% TODO:
% The maximum- and standard deviation variations may indicate that the solution is scalable.

% Magic Instruction Overhead
\section{Magic Instruction Overhead}
\label{sec:analysisexperiment_magicinstructionoverhead}
\ldots

% TODO:
% Magic instruction profiling (point out that, although tests show no larger impact of just a magic instruction with nops, that magic instruction require to exit hardware aided virtualization, JIT-compilation optimization, and to simply interpret the occurence of a magic instruction. Speculate surrounding how this may affect the outcome)

% Benchmark variations
\section{Benchmark Variations}
\label{sec:analysisexperiment_benchmarkvariations}
As visualized in figure \ref{fig:scattersphong}\footnote{Note the sinus-like pattern in the lower parts of the left-hand side scatterplot; it's oscillating performance may be a testimony to the rotation of the model.}, the Phong benchmark (see section \ref{sec:methodologyexperiment_benchmarking}) exhibits erratic variations in it's performance when software rasterized in the \dvttermsimics\ platform.
Each graph presents the performance of the Phong benchmark where each symbol represent a frametime sample of one of the three Phong key figure variations (see section \ref{sec:methodologyexperiment_keyfigurevariations}).
From this visualization we may establish that the performance of the Phong benchmark varies greatly when software rasterized in \dvttermsimics , independant of the Phong benchmark texture texel quantity.

It is uncertain as of what is causing this behaviour.
However, there may be cause to believe that texture mapping a model such as the one in the Phong benchmark, with a non-mipmapped texture of such a large texel count, may induce severe cache-misses due to the volotile texture mapping.
Such a scenario may be induced by the large texture in coagency with the frame-wize rotating model - since the texture mapping outcome will likely differ on a per-frame basis.
In consideration to sheer size of the textures used the experiments presented in figure \ref{fig:scattersphong}, it likely that such eccentric memory referencing may account for the volotile performance of the Phong benchmark in the software rasterized \dvttermsimics\ platform.
Albeit not applicable to \dvttermcpu\ cache bahaviour, see \dvtcmdcitebib{journals:dogget:2012} for an elaboration on texture mapping and \dvttermgpu\ cache methodologies, in addition to an analysis on the performance implications of \dvttermgpu\ cache behaviour.

In line with the erratic benchmark behaviour that has been established in this section, the Phong benchmark results - although having value to this study in terms of profiling memory bandwidth scalability (see paragraph \dvtcmdrefname{par:analysisexperiment_phong}) - are to be considered less reliable than it's more predictable benchmark peers.

\noindent
Note that, for the sake of visualization, values outside of the standard deviation are not presented in figure \ref{fig:scattersphong}.

\input{figscattersphong.tex}

% TODO:
% Consider adding table presenting minimum- and maximum values of these instances.

% Paravirtualization Performance
%\section{Paravirtualization Performance}
%\label{sec:analysisexperiment_paravirtualizationperformance}
%QEMU Vs. Simics.\\
%\ldots

% TODO:
% Add this section to document.
