% thesisanalysisexperiment.tex
% Chapter Analysis, Experiment

% TODO:
% Strengths of solution.
% Identified bottlenecks.
% Speculation surrounding memory page translation.

% Analysis, Experiment
\chapter{Analysis, Experiment}
\label{cha:analysisexperiment}
\ldots

% TODO:
% Analyze the performance of host benchmarks
% Analyze the performance of qemu benchmarks

% Double-peak behaviour in phong demo may be due to rotation of the teapot.

\paragraph{Chess}
\label{par:analysisexperiment_chess}
From the data visualized in figure \ref{fig:histogramssimicsparachess}, we may observe that the Chess benchmark, when executed in the software rasterized \dvttermsimics\ platform, has a relatively broad distribution of it's sample density, yet the distribution often seems evenly distributed around a singe point\todo{Confirm this by analyzing and presenting the benchmark mean.}.
The right-hand side of the graph, although also showcasing the impaired performance of the corresponding - paravirtualized - platform, visualises a decrease in the distribution of the sample density.
This is supported by the data presented in table \ref{tab:keyvalpara}.

Based on the data summarized in table \ref{tab:keyvalsimics} (whilst software rasterized in \dvttermsimics ) and comparing said data to that of table \ref{tab:keyvalpara} (whilst paravirtualized in \dvttermsimics ), we may observe that the software rasterized solution outperforms it's paravirtualized counterpart; not only in the base experiment, but in all of it's variations.
The only redeeming attributes the paravirtualized solution brings to the table, as elaborated upon in the above paragraph, is a decrease in the standard deviation of the benchmark profiling.
When comparing these results to the uncompromised hardware accelerated counterpart on the \dvttermhost\ machine (see figure \ref{fig:histogramshost}), we may observe - albeit considerably less prominent - an adherence to the single-peak behaviour in the destribution of the sample density.

The purpose of the Chess benchmark was to locate any bottlenecks related to the number of paravirtualized library invocations, which was predicted during the pilot study performed for the sake of this experiment (see \dvtcmdcitefur{dissertation:nilsson:2014}).
As such, there is cause to believe that the prediction of a probable bottleneck in the \dvttermtarget - to \dvttermhost\ communication latency has been confirmed; arguably identifying the weakness of graphics paravirtualization in the \dvttermsimics\ full-system simulator.

The conclusion drawn from the Chess benchmark data presented in chapter \ref{cha:results}, stresses further analysis into what is the root cause for the \dvttermtarget - to \dvttermhost\ latency for a multitude of paravirtualized method invocations.
Some suspicions related to this matter are presented in section \ref{sec:analysisexperiment_magicinstructionoverhead}. 

% TODO:
% Mention the number of magic instructions.

\paragraph{Julia}
\label{par:analysisexperiment_julia}
In figure \ref{fig:histogramssimicsparajulia}, we may observe double- to triple peak behaviour in the distribution of the sample density; both in software rasterized- and paravirtualized platforms.
Albeit the hardware accelerated \dvttermhost\ profiling (see figure \ref{fig:histogramshost}) may, however minor, suggest such a pattern; it is by all means not significant.
We may observe similar behaviour in the distribution of the sample density when profiling the same benchmark whilst paravirtualized in the \dvttermqemu -derived \dvttermandroidemulator\ (see figure \ref{fig:histogramsqemu}).
What causes this behaviour is unclear, as frame-to-frame branching in the fractal algorithm is minor and ought not cause such a variance.

The reason as to why the Julia benchmark was decided upon to be integrated into experiment outlined during the pilot study (see \dvtcmdcitefur{dissertation:nilsson:2014}) was to establish how the paravirtualized solution performed under computational stress.
Using this benchmark, a performance weakness in the software rasterized \dvttermsimics\ platform has been identified, with frametimes well above the two second mark (see table \ref{tab:keyvalsimics}); with the corresponding maximum frametime in the paravirtualized \dvttermsimics\ platform measuring up to to \dvtcmdfirstline{parajulia900.dat.max} ms.
With the Julia benchmark, as visualized in figure \ref{fig:histogramssimicsparajulia}, we've showcased radical performance improvements for computationally intensive graphics kernels, and - in turn - possibly identified the capabilities of graphics paravirtualization in the \dvttermsimics\ full-system simulator\todo{Expand upon scalability of Julia paravirtualization.}.

% TODO:
% Percentage-wize, the standard deviation has increased in the paravirtualized platform?
% Scalability
% Visualize how the Julia benchmark performed on the host with variations - in order to establish that the paravirtualized solution scales equally well - being inhibited only by overhead.

\paragraph{Phong}
\label{par:analysisexperiment_phong}
\begin{alltt}
DISCLAIMER: The Phong benchmark signals strange behaviour when software rasterized on the simulation \dvttermhost , and is to be considered less reliable (see section \ref{sec:analysisexperiment_benchmarkvariations}).
\end{alltt}

\noindent
The Phong benchmark was incorporated into this study for the purposes of analyzing the performance of stressed bandwidth in \dvttermtarget - to \dvttermhost\ communications; featuring relaying and rendering of large texture to simulation \dvttermhost .
The graphs presented in figure \ref{fig:histogramssimicsparaphong} display erratic distribution of the sample density in both software rasterized- and paravirtualized \dvttermsimics\ platforms.
Suspicions as to why this is the case are presented in section \ref{sec:analysisexperiment_benchmarkvariations}.
Furthermore, one might argue that parts of the distribution align with the double-peak density distribution showcased by the benchmark when hardware accelerated on the simulation \dvttermhost\ (see figure \ref{fig:histogramshost}).

% TODO:
% Elaborate into what may cause this cost. Memory page translation etc.

By analyzing the data in presented in tables \ref{tab:keyvalsimics} and \ref{tab:keyvalpara}, it is clear that the Phong benchmark, in terms of average framtimes, performs only marginally better or competatively to it's software rasterized equivalent.
However, and more interestingly so, executing the benchmark in the paravirtualized \dvttermsimics\ environment results in major improvements in terms of frametime maximum and standard deviation; topping in at \dvtcmdfirstline{simicsphong1448x1448.dat.max} ms where the corresponding paravirtualized maximum is but \dvtcmdfirstline{paraphong1448x1448.dat.max} ms.

What causes the compiled paravirtualized Phong benchmark samples to be only marginally better or competitively to their the software rasterized counerparts is unclear.
The effect could be related to a bottleneck in the memory page traversal, as described in section \ref{sec:methodologysolution_memorypagetraversal}, signifying a possible weakness in - not only the communicational latency, as established in paragraph \dvtcmdrefname{par:analysisexperiment_chess} - in the memory bandwidth of the Simics Pipe (see section \ref{sec:methodologysolution_simicspipe}).
However, in line with the recorded competatitive average- and minimum frametimes  - yet major improvements in frametime maximum and standard deviation - one might argue that the lightweight benchmark is misrepresentative in that it's software rasterized execution is below that of the overhead induced by paravirtualization.

However, due to the deviances described in section \ref{sec:analysisexperiment_benchmarkvariations}, this will not be elaborated upon further for the remainder of this dissertation.

% TODO:
% The maximum- and standard deviation variations may indicate that the solution is scalable.

% Magic Instruction Overhead
\section{Magic Instruction Overhead}
\label{sec:analysisexperiment_magicinstructionoverhead}
\ldots

% TODO:
% Magic instruction profiling (point out that, although tests show no larger impact of just a magic instruction with nops, that magic instruction require to exit hardware aided virtualization, JIT-compilation optimization, and to simply interpret the occurence of a magic instruction. Speculate surrounding how this may affect the outcome)

% Benchmark variations
\section{Benchmark Variations}
\label{sec:analysisexperiment_benchmarkvariations}
\ldots

% TODO:
% Benchmark Variations
% Benchmark Fluctuations
% Benchmark Volatility

% TODO:
% Expand upon Phong deviations.

%Deviations caused by rotation of the model may cause, or partly cause, general fluctuation in benchmark (e.g. when paravirtualized). Furthermore, the texturing (or the sampling of, rather) of a rotating small model with such a large texture may cause severe cache misses due to the texture sampling. This is simply speculation, but may cause the major fluctuations in the software rasterized Phong benchmark.
%As such, present suspicions of cache misses and general fluctuations in the Phong benchmark, but point out that the software rasterized Phong benchmark profiling is of less relevance to the paravirtualized sample due to this.

% Paravirtualization Performance
\section{Paravirtualization Performance}
\label{sec:analysisexperiment_paravirtualizationperformance}
QEMU Vs. Simics.\\
\ldots