% thesismethodologyexperiment.tex

% Experimental Methodology:
\chapter{Experimental Methodology}
\label{cha:experimentalmethodology}

% Platform Configuration
\section{Platform Configuration}
\label{sec:experimentalmethodology_platformconfiguration}
The experiment devised for the purpose of this dissertation is performed in software rasterized- and paravirtualized \dvttermsimics\ platforms, respectively.
Furthermore, the experiment profiles the performance of the \dvttermreferencesolution\ for good measure, in addition to the performance of the original execution on the hardware accelerated \dvttermhost\ platform.
The following paragraphs outline the configuration of these target platforms, in terms of \dvttermhost\ configuration, simulated hardware, and the software configuration of the simulated systems.

% Host Configuration
\paragraph{Host Configuration}
\label{par:experimentalmethodology_platformconfiguration_hostconfiguration}
The system upon which the experiment is performed is an \dvttermxeightysix -compatible Haswell \dvttermintel\ processor configured to run the \dvttermfedora\ \dvttermlinux\ distribution.
Said \dvttermos\ was selected for use due to \dvttermfedora\ being the primary platform used for development of the \dvttermsimics\ full-system simulator at the \dvttermintel\ offices in Stockholm - at which the solution described in this document has been developed.

% TODO:
% Hardware Specifications:
% CPU: 4x Intel(R) Core(TM) i7-4770 CPU @ 3.40GHz
% RAM: ?
% Software Specifications:
% OS: Fedora 19 (Schr√∂dinger's Cat)
% GL: 3.0 Mesa 9.2.4
% GPU: ?
% Tip: Look up if there are any recommendations as to how one ought to list system configuration for experiments.

% Target Hardware Configuration
\paragraph{Target Hardware Configuration}
\label{par:experimentalmethodology_platformconfiguration:targethardwareconfiguration}
The \dvttermsimics\ hardware configuration utilized for the purpose of this experiment simulates an \dvttermintelcoreiseven ; the same processor series to that of the actual hardware of the simulation \dvttermhost\ system.
Furthermore, \dvttermsimics\ execution, both the software rasterized and paravirtualized platforms, utilize \dvttermkvm\ for the simulation \dvttermtarget\ to run natively on the \dvttermhost\ hardware.

In order to accommodate for similar acceleration in \dvttermqemu , \dvttermandroid\ is configured to run on the \dvttermintel\ \dvttermandroid\ \dvttermxeightysix\ system image (see ~\dvtcmdciteref{technicaldocs:intel:2013:x86}), circumventing the need to interpret \dvttermarm\ instructions~\dvtcmdciteref{web:stylianou:2010}.
Furthermore, the solution also utilize \dvttermkvm\ in order to provide similar native execution on the \dvttermhost\ hardware\footnote{Had the solution been running on the \dvttermwindows\ \dvttermos , the solution would have utilized \dvttermhaxm\ (see ~\dvtcmdciteref{technicaldocs:intel:2013:haxm}) to accommodate for said native execution~\dvtcmdciteref{web:hofemeier:2010}.}.

% Target Software Configuration
\paragraph{Target Software Configuration}
\label{par:experimentalmethodology_platformconfiguration_targetsoftwareconfiguration}
In line with \dvttermfedora\ being the \dvttermos\ in use when performing the reference benchmark experiments on the \dvttermhost\ platform (see paragraph \dvtcmdrefname{par:experimentalmethodology_platformconfiguration_hostconfiguration}), it may be of value to have the simulated \dvttermtarget\ machine configured to run with the same \dvttermos .
As such, the platform \dvttermos s used for the purpose of this study are as follows:
\begin{itemize}[noitemsep]
	\item Hardware accelerated \dvttermfedora\ \dvttermhost\ system
	\item Software rasterized\footnote{Using the Mesa OpenGL software rasterization implementation.} \dvttermfedora\ \dvttermsimics\ \dvttermtarget\ system
	\item Paravirtualized \dvttermfedora\ \dvttermsimics\ \dvttermtarget\ system
	\item \dvttermqemu\ \dvttermandroid\ \dvtcmdnum{4.4} (KitKat, API level 19) \dvttermtarget\ system
\end{itemize}

% Platform Profiling
\section{Platform Profiling}
\label{sec:experimentalmethodology_platformprofiling}
In the presence of the complexities caused by the occurrence of virtual time, profiling of elapsed time in virtual platforms sometimes dictate special measures.
This is due to the fact that profiling of elapsed time outside of the simulation - that is, time in the context of the observer - may be more relevant than profiling of virtual time.
This is often the case if the simulation has outside dependencies of some sort.
Naturally, such is the case in terms of real-time interaction and rendering - as such application is observed from outside of the simulation.

In a \dvttermsimics\ simulation, as described by section \ref{sec:backgroundandrelatedwork_virtualtime}, time increments different than in the context of the \dvttermhost\ machine.
As such, in order to appropriately measure the elapsed time of a benchmark frame, the profiling must take place outside of the simulation.
As expanded upon in section \ref{sec:backgroundandrelatedwork_magicinstructions}, there are a number of ways to escape- and pause the state of a simulation.
Mayhaps the most intelligible is to listen in on activity passing through a \dvttermtarget\ serial port; being a traditional front-end to the machine.
In this way, the simulator is instructed to listen in on, and set a simulation breakpoint at the occurrence of, a specialized sequence of bytes being written - via a serial console - to a \dvttermuart\ hardware serial port.
In \dvttermlinux , the interface to such a port may be accessed by any of the \texttt{/dev/ttyS*} file descriptors.
This process is visualized in figure \ref{fig:profilingsimics}.

\input{figprofilingsimics.tex}

When using serial ports in this manner, one may introduce a profiling cost.
This may be induced by - amongst other possible factors - the file descriptors, to which data is being written, not immediately sending said byte sequence via the system \dvttermuart .
In order to improve responsivity in time critical profiling of elapsed time, we ensure said serial console is flushed to, by the means of performing the system call \dvtcmdcodeinline{tcdrain}.
Furthermore, in order to ensuring the accuracy of performed time profiling, the overhead cost of said method has been measured\footnote{This has been performed by the means of invoking profiling start/stop immediately, with nothing but a \dvtcmdcodeinline{nop}-type instruction in-between.}.
Collected profiling overhead data is presented in table \ref{tab:profiling}.

\input{figprofiling.tex}

From the data presented in table \ref{tab:profiling}, we may deduce that the profiling overhead cost may occasionally be volatile.
However, with a standard deviation of \dvtcmdfirstline{profile.dat.std}, slightly below that of the average, such large deviation is to be quite rare.
If not specified otherwise, all results collected from the \dvttermsimics\ platforms are subtracted with the average of this profiling cost; being \dvtcmdfirstline{profile.dat.avg}\footnote{In some cases, where the overhead cost is to be considered substantial, approximate data is presented accordingly (see section \dvtcmdrefname{sec:appendixa_magicinstructionprofiling} under \dvtcmdrefname{cha:appendixa}).}.
Utilizing said methodology; when the simulator detects the chosen bit-pattern, it may start- or stop platform timing.
This is the method used to measure the elapsed time of a frame during any benchmark running inside of a \dvttermsimics\ simulation.
As such, this method is used for the software rasterized- and paravirtualized \dvttermsimics\ platforms. 

Likewise, in \dvttermqemu , consideration must be taken to virtual time.
However, in the \dvttermqemu -derived \dvttermandroidemulator , the system clock appears synchronized to that of the simulation \dvttermhost \dvtcmdciteref{technicaldocs:google:2014:androidtimers}\dvtcmdciteref{web:diebin:2010}.
The theory is supported by findings presented in \dvtcmdciteref{web:stackoverflow:2011}, and has been confirmed with tests profiling elapsed time before performing any experiments.
It is probably that the \dvttermandroidemulator\ has been implemented in this way to accomodate for audio- and video playback common for Android applications.
As such, in order to profile elapsed wall clock time when performing benchmarks test in the \dvttermandroidemulator , one may simply measure time elapsed inside of the simulation.
This is the methodology used to profile the \dvttermqemu\ platform.

% Benchmarks
\section{Benchmarks}
\label{sec:experimentalmethodology_benchmarks}
Throughout the course of the pilot study, no existing \dvttermopenglestwopointo\ benchmark - with cross-platform profiling support for \dvttermandroid\ and \dvttermxeleven\ \dvttermlinux\ - was deemed appropriate for for the purposes of this dissertation.
As such, a number of benchmarks have been devised on-site for the purposes of stress-testing the solution developed for the purpose of this study.
The benchmarks numbering three in total - they are intended to stress suspected bottlenecks in the implementation; corresponding to a large number of relatively insignificant \dvttermopengl\ invocations, computationally intensive \dvttermgpu\ kernels, and passing of large data such as textures or models.
The benchmarks, intended to run on a \dvttermhost\ \dvttermfedora\ \dvttermlinux\ system, a virtualized \dvttermsimics\ \dvttermfedora\ configuration, and a \dvttermqemu\ \dvttermandroid\ configuration, utilize \dvttermjni\ to invoke \dvttermopengles\ from the same \dvttermc\ code base independent of platform.
Furthermore, all benchmarks have been configured to run at roughly \dvtcmdnum{16}~\milli\second , which would correspond to roughly \dvtcmdnum{60}~\dvttermfps , when hardware accelerated on the \dvttermhost\ system - in order to reflect the expected load of a modern real-time interactive application.
As such, the purpose of developed benchmarks is to be representative of typical scenarios induced by modern \dvttermgui s whilst utilizing a graphics framework such as \dvttermopengl .

Frame captures of the benchmarks are presented in figures \ref{fig:benchmarks_chess}, \ref{fig:benchmarks_julia}, and \ref{fig:benchmarks_phong}.

\input{figbenchmarks.tex}

% TODO:
% * Describe how the benchmark is written in Java, C, and GLSL.
% * Resolution of experiment and other common factors

\paragraph{Benchmark: Chess}
\label{par:experimentalmethodology_benchmarking_benchmarkchess}
\index{Chess benchmark}
The 'Chess' benchmark is developed for the purposes of stressing the latency in-between \dvttermtarget - and \dvttermhost\ systems.
It is so named because of the chess-like tileset the graphics kernel produces.
The benchmark is designed to perform a multitude of \dvttermopenglestwopointo\ library invocations per frame; in which each invocation is relatively lightweight in execution and carry a small amount of data argument-wise.
In the Chess benchmark, this is achieved by rendering a grid of colored (black or white, in order to adhere to the chess paradigm) rectangles where each tile is represented by four two-dimensional vertices in screen-space, in addition to six indices outlining the rectangular shape.
Since the vertices are already transformed into screen-space, the graphics kernel need perform no additional transformation, adhering to the desired lightweight behavior of each kernel invocation.
Additionally, the tileset vertices and indices are pre-loaded into \dvttermopengl\ vertex- and index element buffers, so that a lone buffer identifier may be carried over in-place of the heavier vertex set load.
Each tile is then individually drawn to the backbuffer, rendering the chess-like appearance of the benchmark.
Effectively, this means that, for each tile, the benchmark need only bind a vertex- and an index element buffer, set the corresponding tile color, and lastly invoke the rendering of said tile.

For each frame rendered, depending on the number of drawn tiles, the solution will perform a large number of \dvttermmagicinstruction s.
This induces a high utilization of the Simics Pipe, which is intended to stress \dvttermmagicinstruction\ overhead (section \ref{sec:results_magicinstructionoverhead}).

The repeated invocation of lesser draw calls is representative of common usage of drawing a multitude of shapes with \dvttermopengl , such as a user interface. Additionally, the number of tiles being computed is easily modifiable; rendering the benchmark scalable for the purposes of the experiment described in this document. As such, said benchmark is considered suitable for the purpose of representing a large number of graphics invocations using \dvttermopenglestwopointo .

\paragraph{Benchmark: Julia}
\label{par:experimentalmethodology_benchmarking_benchmarkjulia}
\index{Julia fractal benchmark}
The 'Julia' benchmark is developed for the purposes of stressing computational intensity in software-rasterized and paravirtualized platforms.
It is so named due to the kernel calculating the Julia fractal; the texturing and frame-wise seeding of which gives the benchmark it's distinct look.
The benchmark is designed to perform a lone computationally intensive graphics kernel invocation, which will stress the computational prowess of the profiled platform.
The case is selected for use as the computation of a fractal is trivially scalable in terms of complexity by modifying the number of iterations the fractal algorithm performs, and is thus considered suitable for profiling of computationally intensive graphics kernels.

\paragraph{Benchmark: Phong}
\label{par:experimentalmethodology_benchmarking_benchmarkphong}
\index{Phong shading benchmark}
The 'Phong' benchmark is developed for the purposes of stressing the throughput, or bandwidth - if adhering to the networking paradigm, in-between \dvttermtarget - and \dvttermhost\ systems.
The Phong benchmark is comprised of a rotating model, being the Newell teapot, which is textured and subsequently shaded by a single point light using the Phong shading model; thus giving the benchmark it's name. % \todo{point out as to why the newell teapot is a standard model}

The rasterization and shading of a model with a given, large, texture is representative of three-dimensional graphics commonly rendered with graphics frameworks such as \dvttermopenglestwopointo .
As such, said benchmark is suitable for the purposes of representing the usage of big data (being models, textures, etc.).
For the purposes of stressing the bandwidth of \dvttermtarget - and \dvttermhost\ communication, the Phong benchmark is easily scalable in terms of resizing the large texture in question.
Vertices and indices of the model do not utilize \dvttermopengl\ vertex- and-/or index element buffers, thus forcing the solution to transmit the data every frame.
Additionally, in order to further stress the throughput of the profiled platform, the large texture is updated every frame.

% Input Data Variation
\section{Input Data Variation}
\label{sec:experimentalmethodology_inputdatavariation}
In order to detect anything but linear potential of scaling in software rasterized- and paravirtualized \dvttermsimics\ platforms, each benchmark performed in said platforms has been executed in three different versions, respectively.
These benchmark versions differ in terms of a key figure; where said key figure is a changed variable that could potentially improve- or worsen benchmark performance.
The purpose of these 'key figure variations' is to detect any unexpected results in execution; and thus identify any performance complexity issues in software rasterized- and paravirtualized \dvttermsimics\ platforms.
For consistency, each benchmark variation - of which there are two; resulting in three unique experiments per benchmark - has been produced to halve- and subsequently double the corresponding key figure.

The described key figures, for each benchmark, are presented in table \ref{tab:keyvals}.
In table \ref{tab:keyvalsmagicinstructions}, the per-frame number of magic instructions induced by these key figure variations are presented.

% fig:keyvals
\input{figkeyvals}

% fig:keyvalsmagicinstructions
\input{figkeyvalsmagicinstructions}

Note that for each tile rendered in the Chess benchmark, the solution will perform $9$ \dvttermmagicinstruction s; entailing a total of $32400$\footnote{$9\times60\times60=32400$.}, $63504$\footnote{$9\times84\times84=63504$.}, and $125316$\footnote{$9\times118\times118=125316$.} \dvttermmagicinstruction s, respectively, in addition to a minor number per-frame.

% Threats to Validity
\section{Threats to Validity}
\label{sec:experimentalmethodology_threatstovalidity}
\ldots
