% thesismethodologyexperiment.tex
% Chapter Methodology, Experiment.

% Methodology, Experiment:
\chapter{Methodology, Experiment}
\label{cha:methodologyexperiment}
\ldots

% TODO:
% Describe suspicions of bottlenecks and the purposes of the benchmarks described in sections further down below.

%It is vital to sound experiment methodology that the tests performed throughout the benchmark focuses on critical areas and suspected bottlenecks in the implementation.
%Such key points during simulation may concern a large number of relatively insignificant \termopengles\ invocations, frame-wize \termopengles\ state saving, or transferral of large chunks of data such as textures.
%These areas may very well be bottlenecks of graphics paravirtualization and thus call for further investigation.
%Due to the proposed study accentuating performance, and since it concerns a graphics framework often used with real-time applications, it may be viable to measure experimental data in \termframespersecond ~(\termfps ).
%Alternatively, lone dispatch measurements could be used in coagency with \termframespersecond\ measurements.
%Additionally, it may be favorable if, during development of said benchmark, if the software could utilize the same code-base to invoke the \termopengles -libraries - independant of platform.

\section{Platform Configuration}
\label{sec:methodologyexperiment_platformconfiguration}
The experiment devised for the purpose of this dissertation is performed on the platforms outlined in \dvtcmdcitefur{dissertation:nilsson:2014}; being software rasterized- and paravirtualized \dvttermsimics , respectively.
Furthermore, the experiment profiles the performance of the \dvttermreferencesolution\ for good measure, in addition to the performance of the original execution on the hardware accelerated \dvttermhost\ platform.

The following paragraphs outline the configuration of these target platforms, in terms of \dvttermhost\ configuration, simulated hardware, and the software configuration of the simulated systems.

\paragraph{Host Configuration}
\label{par:methodologyexperiment_platformconfiguration_hostconfiguration}
The system upon which the experiment is performed is an \dvttermxeightysix -compatible \dvttermintel\ processor configured to run the \dvttermfedora\ \dvttermlinux\ distribution.
Said \dvttermos\ was selected for use due to \dvttermfedora\ being the primary platform used for development of the \dvttermsimics\ full-system simulator at the \dvttermintel\ offices in Stockholm - at which the solution described in this document has been developed.

% TODO:
% Hardware Specifications:
% CPU: 4x Intel(R) Core(TM) i7-4770 CPU @ 3.40GHz
% RAM: ?
% Software Specifications:
% OS: Fedora 19 (Schr√∂dinger's Cat)
% GL: 3.0 Mesa 9.2.4
% GPU: ?
% Tip: Look up if there are any recommendations as to how one ought to list system configuration for experiments.

\paragraph{Target Hardware Configuration}
\label{par:methodologyexperiment_platformconfiguration:targethardwareconfiguration}
The \dvttermsimics\ hardware configuration utilized for the purpose of this experiment simulates an \dvttermintelcoreiseven ; the same processor series to that of the actual hardware of the simulation \dvttermhost\ system.
Furthermore, \dvttermsimics\ execution, both the software rasterized and paravirtualized platforms, utilize \dvttermkvm\ for the simulation \dvttermtarget\ to run natively on the \dvttermhost\ hardware.

In order to accomodate for similar acceleration in \dvttermqemu , \dvttermandroid\ is configured to run on the \dvttermintel\ \dvttermandroid\ \dvttermxeightysix\ system image (see \dvtcmdciteref{technicaldocs:intel:2013:x86}), circumventing the need to interpret \dvttermarm\ instructions\dvtcmdciteref{web:stylianou:2010}.
Furthermore, the solution utilizes \dvttermhaxm\ (see \dvtcmdciteref{technicaldocs:intel:2013:haxm}) in order to provide similar native execution on the \dvttermhost\ hardware\dvtcmdciteref{web:hofemeier:2010}\todo{Are we using KVM or HAXM to accelerate QEMU simulation?} .

\paragraph{Target Software Configuration}
\label{par:methodologyexperiment_platformconfiguration_targetsoftwareconfiguration}
In line with \dvttermfedora\ being the \dvttermos\ in use when performing the reference benchmark experiments on the \dvttermhost\ platform (see paragraph \dvtcmdrefname{par:methodologyexperiment_platformconfiguration_hostconfiguration}), it may be of value to have the simulated \dvttermtarget\ machine configured to run with the same \dvttermos .
As such, the platform \dvttermos s used for the purpose of this study are as follows: hardware accelerated \dvttermfedora\ \dvttermhost\ system, software rasterized \dvttermfedora\ \dvttermsimics\ \dvttermtarget\ system, paravirtualized \dvttermfedora\ \dvttermsimics\ \dvttermtarget\ system, and \dvttermqemu\ \dvttermandroid\ \dvtcmdnum{4.4} (KitKat, API level 19) \dvttermtarget\ system.

% TODO:
% What software rasterizer is Simics using (Mesa)

% Platform Profiling
\section{Platform Profiling}
\label{sec:methodologyexperiment_platformprofiling}
In the presence of of the complexities caused by the occurence of virtual time (see section \dvtcmdrefname{sec:appendixa_virtualtime} under \dvtcmdrefname{cha:appendixa}), profiling of time in virtual platforms sometimes dictate special measures.
This is due to the fact that sometimes, if the simulation has outside dependencies of some sort, elapsed time outside of the simulation - that is, time in the pretext of the observer - is more relevant than profiling virtual time.
Naturally, such is the case in terms of real-time interaction and rendering - as such application is observed from outside of the simulation.

In a \dvttermsimics\ simulation, as described by section \dvtcmdrefname{sec:appendixa_virtualtime} under \dvtcmdrefname{cha:appendixa}, time fleets differently than in the context of the \dvttermhost\ machine.
As such, in order to appropriately measure the elapsed time of a benchmark frame, said profiling must take place outside of the simulation.
As expanded upon in section \dvtcmdrefname{sec:appendixa_magicinstructions}, there are a number of ways to escape- and pause the state of a simulation.
Mayhaps the most intelligible is to instruct the simulator to listen in on, and set a simulation breakpoint at the occurence of, a specialized sequence of bytes 

% Serial Console
% UART

Likewize, in \dvttermqemu ...
However,

% TODO:
% Timing methodologies:
% * In Linux, we may simply...
% * Accomodating for virtual time in Simics and QEMU Android.

% Benchmarking
\section{Benchmarking}
\label{sec:methodologyexperiment_benchmarking}
Throughout the course of the pilot study, no existing \dvttermopenglestwopointo\ benchmark - with cross-platform profiling support for \dvttermandroid\ and \dvttermxeleven\ \dvttermlinux\ - was deemed appropriate for for the purposes of this dissertation (see \dvtcmdcitefur{dissertation:nilsson:2014} for an elaboration on said inquiry).
As such, a benchmark has been devised on-site for the purposes of stress-testing the solution developed for the purpose of this study.
The benchmark consists of three seperate tests; each intended to stress suspected bottlenecks in the implementation; corresponding to a large number of relatively insignificant \dvttermopengl\ invocations, computationally intensive \dvttermgpu\ kernels, and passing of large data such as textures or models (see \dvtcmdcitefur{dissertation:nilsson:2014} for an elaboration on the ambitions and motives for said benchmark).
The benchmark, intended to run on a \dvttermhost\ \dvttermfedora\ \dvttermlinux\ system, a virtualized \dvttermsimics\ \dvttermfedora\ configuration, and a \dvttermqemu\ \dvttermandroid\ configuration, utilizes \dvttermjni\ to invoke \dvttermopengles\ from the same \dvttermc\ code base independent of platform; in line with the motives outlined in \dvtcmdcitefur{dissertation:nilsson:2014}.
Furthermore, all benchmarks have been configured to run at roughly \dvtcmdnum{16}~\milli\second\ (or roughly \dvtcmdnum{60}~\dvttermfps ) when hardware accelerated on the \dvttermhost\ system, in order to reflect the expected load of a modern real-time interactive system.
As such, the purpose of developed benchmarks is to be representative of typical scenarios induced by modern \dvttermgui s whilst utilizing a graphics framework such as \dvttermopengl .

The benchmarks are presented below:

% TODO:
% * Describe how the benchmark is written in Java, C, and GLSL.
% * Resolution of experiment and other common factors

% TODO: Preferably the benchmark images ought to be presented side-by-side, as to not take up too much space (remember, not vector graphics goodies).
\missingfigure[figwidth=6cm]{Side-by-side image of Chess benchmark output.} % TODO: Insert image of benchmark output.
\missingfigure[figwidth=6cm]{Side-by-side image of Chess benchmark output.} % TODO: Insert image of benchmark output.
\missingfigure[figwidth=6cm]{Side-by-side image of Chess benchmark output.} % TODO: Insert image of benchmark output.

\paragraph{Benchmark: Chess}
\label{par:methodologyexperiment_benchmarking_benchmarkchess}
\index{Chess benchmark}
The 'Chess' benchmark is developed for the purposes of stressing the latency in-between \dvttermtarget - and \dvttermhost\ systems.
It is so named because of the chess-like tileset the graphics kernel produces (\todo{refer to figure}).
The benchmark is designed to perform a multitude of \dvttermopenglestwopointo\ library invocations per frame; in which each invocation is relatively lightweight in execution and carry a small amount of data argument-wize.
In the Chess benchmark, this is achieved by renderering a grid of colored (black or white, in order to adhere to the chess paradigm) rectangles where each tile is represented by four two-dimensional vertices in screen-space, in addition to six indices outlining the rectangular shape.
Since the vertices are already transformed into screen-space, the graphics kernel need perform no additional transformation, adhering to the desired lightweight behaviour of each kernel invocation.
Additionally, the tileset vertices and indices are pre-loaded into \dvttermopengl\ vertex- and index element buffers, so that a lone buffer identifier may be carried over in-place of the heavier vertex set load.
Each tile is then individually drawn to the backbuffer, rendering the chess-like appearance of the benchmark.

Effectively, this means that, for each tile, the benchmark need only bind a vertex- and an index element buffer, set the corresponding tile color, and lastly invoke the rendering of said tile.
The benchmark kernel in it's entirety is presented in section \dvtcmdrefname{sec:appendixb_benchmarkchesskernel} under \dvtcmdrefname{cha:appendixb}.

The repeated invocation of lesser draw calls is representative of common usage of drawing a multide of shapes with \dvttermopengl , such as a user interface. Additionally, the number of tiles being computed is easily modifiable; rendering the benchmark scalable for the purposes of the experiment described in this document. As such, said benchmark is considered suitable for the purpose of representing a large number of graphics invocations using \dvttermopenglestwopointo .

\paragraph{Benchmark: Julia}
\label{par:methodologyexperiment_benchmarking_benchmarkjulia}
\index{Julia fractal benchmark}
The 'Julia' benchmark is developed for the purposes of stressing computational intensity in software-rasterized and paravirtualized platforms.
It is so named due to the kernel calculating the Julia fractal (\todo{refer to figure}); the texturing and frame-wize seeding of which gives the benchmark it's distinct look.
The benchmark is designed to perform a lone computationally intensive graphics kernel invocation, which will stress the computational prowess of the profiled platform.
The case is selected for use as the computation of a fractal is trivially scalable in terms of complexity by modifying the number of iterations the fractal algorithm performs, and is thus considered suitable for profiling of computationally intensive graphics kernels.
Then benchark kernel in it's entirety is presented in section
\dvtcmdrefname{sec:appendixb_benchmarkjuliakernel} under \dvtcmdrefname{cha:appendixb}.

\paragraph{Benchmark: Phong}
\label{par:methodologyexperiment_benchmarking_benchmarkphong}
\index{Phong shading benchmark}
The 'Phong' benchmark is developed for the purposes of stressing the throughput, or bandwidth - if adhering to the networking paradigm presented in paragraph \dvtcmdrefname{par:methodologyexperiment_benchmarking_benchmarkchess}, in-between \dvttermtarget - and \dvttermhost\ systems.
The Phong benchmark is comprised of a rotating model, being the Newell teapot\todo{point out as to why the newell teapot is a standard model}, which is textured and subsequently shaded by a single point light using the Phong shading model; thus giving the benchmark it's name (\todo{refer to figure}).

The rasterization and shading of a model with a given, large, texture is representative of three-dimensional graphics commonly rendered with graphics frameworks such as \dvttermopenglestwopointo .
As such, said benchmark is suitable for the purposes of representing the usage of big data (being models, textures, etc.).
For the purposes of stressing the bandwidth of \dvttermtarget - and \dvttermhost\ communication, the Phong benchmark is easily scalable in terms of resizing the large texture in question.
Vertices and indices of the model do not utilize \dvttermopengl\ vertex- and-/or index element buffers, thus forcing the solution to transmit the data every frame.
Additionally, in order to further stress the throughput of the profiled platform, the large texture is updated every frame.
The kernel source code, in it's entirety, is presented in section \dvtcmdrefname{sec:appendixb_benchmarkphongkernel} under \dvtcmdrefname{cha:appendixb}.
