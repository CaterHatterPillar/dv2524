% thesisresults.tex
% Chapter Results

% Results
\chapter{Results}
\label{cha:results}
The benchmark configurations, as described in section \ref{sec:methodologyexperiment_benchmarking}, having been executed on the platforms, as described in section \ref{sec:methodologyexperiment_platformconfiguration}, using the profiling methods described in section \ref{sec:methodologyexperiment_platformprofiling}, are presented in this chapter.
As such, the results gathered from execution on the \dvttermhost\ is presented in figure \ref{fig:histogramshost}, the results compiled from execution in the \dvttermandroidemulator\ presented in figure \ref{fig:histogramsqemu}, and the results accumulated from software rasterized- and paravirtualized execution in \dvttermsimics\ are presented in figures \ref{fig:histogramssimicsparachess}, \ref{fig:histogramssimicsparajulia}, and \ref{fig:histogramssimicsparaphong}, respectively.

Note that the data compiled from software rasterized- and paravirtualized \dvttermsimics\ platforms also include data form the key figure variation experiments, as described in section \ref{sec:methodologyexperiment_keyfigurevariations}.\\

\noindent
In this chapter, the results are compiled into histograms; visualizing elapsed time in milliseconds to sample density.
As such, the $Y$ axis showcases sample density; although the axis keys have been removed as they bear little relevance to the outcomes presented in this document.
The histograms each feature \dvtcmdnum{100} bins; into which a 1000 samples, for each experiment performed, are rounded into.
For the purposes of good visualization methodology, values outside of the standard deviation\footnote{That is; values above that of the $mean + std$ and values under that of $mean - std$.} are not featured in the figures presented in this section.
In order to accomodate for the, however few, samples outside of said limits the figures are all complemented with key ratio tables (see tables \ref{tab:keyvalhost}, \ref{tab:keyvalqemu}, \ref{tab:keyvalsimics}, and \ref{tab:keyvalpara}, respectively).\\

\noindent
Based off the reference profiling presented in figure \ref{fig:histogramshost}, we may conclude that the benchmarks, when hardware accelerated on the \dvttermhost\ system, perform with concentrated density; not being much scattered across the graph except a few irregular extremeties in terms of maximum frametimes (see table \ref{tab:keyvalhost}).
This is supported by the standard deviation presented in said table.

Furthermore, we may conclude that the Phong demo features two distinct peaks in density distribution - about $1$ ms in-between.
There is cause to believe that this may be caused, or partly caused, by frame-wize rotation of the teapot - rotation featured in the benchmark (see section \ref{sec:methodologyexperiment_benchmarking}) - inducing some fluctutation into it's execution (see figure \ref{fig:histogramssimicsparaphong}).
However, this behaviour is, strangely so, not apperent whilst paravirtualized in the \dvttermqemu\ derived \dvttermandroidemulator , although this may be a visual artifact due to the resolution of the graph (see figure \ref{fig:histogramsqemu}).
See section \ref{sec:results_benchmarkvariations} for an elaboration on divergency in the Phong benchmark.

Additionally, and in accordance to tables \ref{tab:keyvalhost} and \ref{tab:keyvalqemu}, one may observe relatively high recorded maximum frametimes in relation to compiled maximum- and average values, yet featuring - in relation to divergent maximum, relatively low standard deviations.\\

\noindent
The remainder of this chapter will present a benchmark-wize analysis of the data compiled from executing the experiment on the software rasterized- and paravirtualized \dvttermsimics\ platforms; said platforms being the subject of this study.
For the sake of brevity, these analyses are segmented into paragraphs for each benchmark.
These paragraphs are presented below.

% histogramshost
\input{fighistogramshost.tex}

% histogramsqemu
\input{fighistogramsqemu.tex}

% keyvalhost
\input{tabkeyvalhost.tex}

% keyvalqemu
\input{tabkeyvalqemu.tex}

% keyvalsimics
\input{tabkeyvalsimics.tex}

% keyvalpara
\input{tabkeyvalpara.tex}

\paragraph{Chess}
\label{par:results_chess}
From the data visualized in figure \ref{fig:histogramssimicsparachess}, we may observe that the Chess benchmark, when executed in the software rasterized \dvttermsimics\ platform, has a relatively broad distribution of it's sample density, yet the distribution often seems evenly distributed around a singe point\todo{Confirm this by analyzing and presenting the benchmark mean.}.
The right-hand side of the graph, although also showcasing the impaired performance of the corresponding - paravirtualized - platform, visualises a decrease in the distribution of the sample density.
This is supported by the data presented in table \ref{tab:keyvalpara}.

Based on the data summarized in table \ref{tab:keyvalsimics} (whilst software rasterized in \dvttermsimics ) and comparing said data to that of table \ref{tab:keyvalpara} (whilst paravirtualized in \dvttermsimics ), we may observe that the software rasterized solution outperforms it's paravirtualized counterpart; not only in the base experiment, but in all of it's variations.
The only redeeming attributes the paravirtualized solution brings to the table, as elaborated upon in the above paragraph, is a decrease in the standard deviation of the benchmark profiling.
When comparing these results to the uncompromised hardware accelerated counterpart on the \dvttermhost\ machine (see figure \ref{fig:histogramshost}), we may observe - albeit considerably less prominent - an adherence to the single-peak behaviour in the destribution of the sample density.

The purpose of the Chess benchmark was to locate any bottlenecks related to the number of paravirtualized library invocations (see section \ref{sec:methodologyexperiment_benchmarking}), which was predicted during the pilot study performed for the sake of this experiment (see \dvtcmdcitefur{dissertation:nilsson:2014}).
As such, there is cause to believe that the prediction of a probable bottleneck in the \dvttermtarget - to \dvttermhost\ communication latency has been confirmed; arguably identifying the weakness of graphics paravirtualization in the \dvttermsimics\ full-system simulator.

The conclusion drawn from the Chess benchmark data presented in chapter \ref{cha:results}, stresses further analysis into what is the root cause for the \dvttermtarget - to \dvttermhost\ latency for a multitude of paravirtualized method invocations.
Some suspicions related to this matter are presented in section \ref{sec:results_magicinstructionoverhead}. 

% TODO:
% Mention the number of magic instructions.

% histogramssimicsparachess
\input{fighistogramssimicsparachess.tex}

\paragraph{Julia}
\label{par:results_julia}
In figure \ref{fig:histogramssimicsparajulia}, we may observe double- to triple peak behaviour in the distribution of the sample density; both in software rasterized- and paravirtualized platforms.
Albeit the hardware accelerated \dvttermhost\ profiling (see figure \ref{fig:histogramshost}) may, however minor, suggest such a pattern; it is by all means not significant.
We may observe similar behaviour in the distribution of the sample density when profiling the same benchmark whilst paravirtualized in the \dvttermqemu -derived \dvttermandroidemulator\ (see figure \ref{fig:histogramsqemu}).
What causes this behaviour is unclear, as frame-to-frame branching in the fractal algorithm is minor and ought not cause such a variance.

The reason as to why the Julia benchmark was decided upon to be integrated into experiment outlined during the pilot study (see \dvtcmdcitefur{dissertation:nilsson:2014}) was to establish how the paravirtualized solution performed under computational stress (see section \ref{sec:methodologyexperiment_benchmarking}).
Using this benchmark, a performance weakness in the software rasterized \dvttermsimics\ platform has been identified, with frametimes well above the two second mark (see table \ref{tab:keyvalsimics}); with the corresponding maximum frametime in the paravirtualized \dvttermsimics\ platform measuring up to to \dvtcmdfirstline{parajulia900.dat.max} ms.
With the Julia benchmark, as visualized in figure \ref{fig:histogramssimicsparajulia}, we've showcased radical performance improvements for computationally intensive graphics kernerls and - in turn - identified the capabilities of graphics paravirtualization in the \dvttermsimics\ full-system simulator\todo{Expand upon scalability of Julia paravirtualization.}.

% TODO:
% Percentage-wize, the standard deviation has increased in the paravirtualized platform?
% Scalability
% Visualize how the Julia benchmark performed on the host with variations - in order to establish that the paravirtualized solution scales equally well - being inhibited only by overhead.

% histogramssimicsparajulia
\input{fighistogramssimicsparajulia.tex}

\paragraph{Phong}
\label{par:results_phong}
\begin{alltt}
DISCLAIMER: The Phong benchmark signals strange behaviour when software rasterized in \dvttermsimics , and is thus considered less reliable (see section \ref{sec:results_benchmarkvariations}).
\end{alltt}

\noindent
The Phong benchmark was incorporated into this study for the purposes of analyzing the performance of stressed bandwidth in \dvttermtarget - to \dvttermhost\ communications; featuring relaying and rendering of a large texture to simulation \dvttermhost .
The graphs presented in figure \ref{fig:histogramssimicsparaphong} display erratic distribution of the sample density in both software rasterized- and paravirtualized \dvttermsimics\ platforms.
Suspicions as to why this is the case are presented in section \ref{sec:results_benchmarkvariations}.

Furthermore, one might argue that parts of the distribution align with the double-peak density distribution showcased by the benchmark when hardware accelerated on the simulation \dvttermhost\ (see figure \ref{fig:histogramshost}).

% TODO:
% Elaborate into what may cause this cost. Memory page translation etc.

By analyzing the data in presented in tables \ref{tab:keyvalsimics} and \ref{tab:keyvalpara}, it is clear that the Phong benchmark, in terms of average frametimes, performs only marginally better or competatively to it's software rasterized equivalent.
However, and mayhaps more interestingly so, executing the benchmark in the paravirtualized \dvttermsimics\ environment results in major improvements in terms of frametime maximum and standard deviation; software rasterized samples topping in at \dvtcmdfirstline{simicsphong1448x1448.dat.max} ms where the corresponding paravirtualized maximum is but \dvtcmdfirstline{paraphong1448x1448.dat.max} ms.\\

\noindent
What causes the compiled paravirtualized Phong benchmark samples to be only marginally better or competitively to their the software rasterized counterparts is unclear.
The effect could be related to a bottleneck in the memory page traversal, as described in section \ref{sec:methodologysolution_memorytabletraversal}, signifying a possible weakness in - in addition to the communicational latency, as established in paragraph \dvtcmdrefname{par:results_chess} - the memory bandwidth of the Simics Pipe (see section \ref{sec:methodologysolution_simicspipe}).

However, in line with the recorded competatitive average- and minimum frametimes - yet major improvements in frametime maximum and standard deviation - one might also argue that the relatively lightweight benchmark is misrepresentative in that it's software rasterized execution is below that of the overhead induced by paravirtualization.

However, due to the deviances described in section \ref{sec:results_benchmarkvariations}, this will not be elaborated upon further for the remainder of this dissertation.

% TODO:
% The maximum- and standard deviation variations may indicate that the solution is scalable.

% histogramssimicsparaphong
\input{fighistogramssimicsparaphong.tex}

% Magic Instruction Overhead
\section{Magic Instruction Overhead}
\label{sec:results_magicinstructionoverhead}
In \dvttermsimics , \dvttermmagicinstruction s incur a context switch when exiting the simulation and beginning execution in the real world.
This affects the performance by forcing the simulation to no longer be executed in native mode; inhibiting the massive simulatory performance imrovements given by \dvttermhostvirtualizationextensions .
Additionally, this also entail \dvttermsimics\ no longer being able to utilize \dvttermjit\ compilation to speed up execution; having to rely on regular code interpretation.
As such, in great numbers, \dvttermmagicinstruction s may potentially affect potentiallyerformance. 

In line with the effect that this overhead may have on experiments such as benchmark Chess (see paragraph \dvtcmdrefname{par:results_phong}), a number of tests have been executed for the purposes of trying establish the obscure overhead \dvttermmagicinstruction s may induce.
These tests have been run in two instances; one in which each \dvttermmagicinstruction\ invocation is profiled seperately using the methodology described in section \ref{sec:methodologyexperiment_platformprofiling}, and another in which batches of \dvttermmagicinstruction\ invocations are profiled.
This has been performed in this manner due to the overload of profiling described in section \ref{sec:methodologyexperiment_platformprofiling}.

\paragraph{Profile each}
\label{par:results_magicinstructionoverhead_profileeach}
The per-invocation profiling of $1000$ \dvttermmagicinstruction s is presented in figure \ref{fig:histogrammagicinstructions} as a histogram.
All samples compiled in said test have been subtracted with the established average performance, as described in section \ref{sec:methodologyexperiment_platformprofiling}.
In line with this modification of the source data, due to the established profiling average overhead of \dvtcmdfirstline{profile.dat.avg} having relatively much influence on the data, the visualization features only ten histogram bins - in coagency with the approximate data.
As such, any offset values below that of zero are placed in the first bin.
Furthermore, any values outside that of the standard deviation are not visualized in this figure.

\input{fighistogrammagicinstructions.tex}

From this data, it is apperent that roughly $45\%$ of profiled samples fall in the interval of $0$-$0.5$ milliseconds, a relatively high measurement compared to that of the batch profiling (see paragraph \dvtcmdrefname{par:results_magicinstructionoverhead_profileall}).
The figure is complemented with the analyzed data presented in table \ref{tab:magicinstructions}

\input{tabmagicinstructions.tex}

\paragraph{Profile all}
\label{par:results_magicinstructionoverhead_profileall}
In order to complement the data presented in paragraph \dvtcmdrefname{par:results_magicinstructionoverhead_profileeach}, corresponding measurements for batch \dvttermmagicinstruction\ invocations is collected and presented in figure...
As such, the 

\noindent
From the data presented in paragraphcs \dvtcmdrefname{par:results_magicinstructionoverhead_profileeach} and \dvtcmdrefname{par:results_magicinstructionoverhead_profileall}, we may establish that...

% Benchmark variations
\section{Benchmark Variations}
\label{sec:results_benchmarkvariations}
As visualized in figure \ref{fig:scattersphong}\footnote{Note the sinus-like pattern in the lower parts of the left-hand side scatterplot; it's oscillating performance may be a testimony to the rotation of the model.}, the Phong benchmark (see section \ref{sec:methodologyexperiment_benchmarking}) exhibits erratic variations in it's performance when software rasterized in the \dvttermsimics\ platform.
Each graph presents the performance of the Phong benchmark where each symbol represent a frametime sample of one of the three Phong key figure variations (see section \ref{sec:methodologyexperiment_keyfigurevariations}).
From this visualization we may establish that the performance of the Phong benchmark varies greatly when software rasterized in \dvttermsimics , independant of the Phong benchmark texture texel quantity.

It is uncertain as of what is causing this behaviour.
However, there may be cause to believe that texture mapping a model such as the one in the Phong benchmark, with a non-mipmapped texture of such a large texel count, may induce severe cache-misses due to the volotile texture mapping.
Such a scenario may be induced by the large texture in coagency with the frame-wize rotating model - since the texture mapping outcome will likely differ on a per-frame basis.
In consideration to sheer size of the textures used the experiments presented in figure \ref{fig:scattersphong}, it likely that such eccentric memory referencing may account for the volotile performance of the Phong benchmark in the software rasterized \dvttermsimics\ platform.
Albeit not applicable to \dvttermcpu\ cache bahaviour, see \dvtcmdcitebib{journals:dogget:2012} for an elaboration on texture mapping and \dvttermgpu\ cache methodologies, in addition to an analysis on the performance implications of \dvttermgpu\ cache behaviour.

In line with the erratic benchmark behaviour that has been established in this section, the Phong benchmark results - although having value to this study in terms of profiling memory bandwidth scalability (see paragraph \dvtcmdrefname{par:analysisexperiment_phong}) - are to be considered less reliable than it's more predictable benchmark peers.

\noindent
Note that, for the sake of visualization, values outside of the standard deviation are not presented in figure \ref{fig:scattersphong}.

\input{figscattersphong.tex}

% TODO:
% Consider adding table presenting minimum- and maximum values of these instances.

% Paravirtualization Performance
%\section{Paravirtualization Performance}
%\label{sec:analysisexperiment_paravirtualizationperformance}
%QEMU Vs. Simics.\\
%\ldots

% TODO:
% Add this section to document.
